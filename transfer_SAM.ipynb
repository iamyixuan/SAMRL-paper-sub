{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferring to the full SAM model\n",
    "Once the actor and critic are trained using the reduced order Sindy model, we can load the weights and optimizer states and continue training on the full SAM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpi4py import MPI\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lstm_ppo import PPOAgent as LSTMPPOAgent\n",
    "from ppo import PPOAgent\n",
    "from ghfr_gym_rd import SINDyGYM\n",
    "from mpi_utils import proc_id, mpi_avg\n",
    "from model_sindy_gfhr import sindyDx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model weights and optimizer states from the previous training and initialize the environment\n",
    "To continue training from the last saved point, we need to resume the policy and value networks' weights as well as the states of their optimizers. \n",
    "\n",
    "There are a few steps:\n",
    "1. Load configurations, including the previous command line arguments and agent hyperparameters.\n",
    "2. Initialize the environment which can be replaced by the full SAM model.\n",
    "3. Initialize and build the policy and value networks.\n",
    "4. Initialize Lagrangians and its optimizer.\n",
    "5. **IMPORTANT:** make all optimizers apply zero gradients first and then load the optimizer states for each optimizer.\n",
    "6. Load the policy and value network pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yixuan/miniforge3/envs/deeplearning/lib/python3.8/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator Pipeline from version 0.24.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(1, 2) dtype=float32, numpy=array([[0.507272 , 0.4391168]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model configs with keys ['arch', 'agent', 'args']\n",
    "MODEL_PATH = './Data/Bebop/Data/LSTM_PPO_KNL_48workers_94-03-30-2023-09-41-37/'\n",
    "with open(MODEL_PATH + 'config', 'rb') as file:\n",
    "    config = pickle.load(file)\n",
    "args = config['args']\n",
    "agent_config = config['agent']\n",
    "\n",
    "if proc_id()==0:\n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"-%m-%d-%Y-%H-%M-%S\")\n",
    "    folder = args.expid+'_'+str(np.random.randint(0,100))+date_time\n",
    "    path = \"./Data/\"+folder\n",
    "    os.mkdir(path)\n",
    "\n",
    "\n",
    "# create the enviornment - to be replaced by the full SAM model.\n",
    "env = SINDyGYM(hist_len=args.histlen,skip=5,rem_time=False, time_independent=True)#skip 10 for the other model\n",
    "\n",
    "# define the model according to the configs\n",
    "control_type = 'continuous'\n",
    "obs_dim = env.state_dim\n",
    "act_dim = env.action_dim*2\n",
    "\n",
    "if args.safety_level>0:\n",
    "    args.learn_cost_critic = False\n",
    "\n",
    "if args.learn_cost_critic:\n",
    "    rew_dim = 3\n",
    "else:\n",
    "    rew_dim = 1\n",
    "delta = args.delta\n",
    "\n",
    "seed = proc_id()\n",
    "\n",
    "neurons = args.neurons\n",
    "layers = args.layers\n",
    "mlp_arch={'pi_layers':[neurons]*layers,'v_repr_layers':[],'v_layers':[neurons]*layers, 'h_size':neurons,'batch_size':args.batch_size}\n",
    "safety_layer={'safety_level':args.safety_level}\n",
    "if safety_layer['safety_level']>0:\n",
    "    dynamics_model = sindyDx()\n",
    "    safety_layer['dynamics_model'] = dynamics_model\n",
    "\n",
    "if args.lstm_actor:\n",
    "    h_size = mlp_arch['h_size']\n",
    "    batch_size = mlp_arch['batch_size']\n",
    "    trace_len = args.env_steps//batch_size\n",
    "    agent = LSTMPPOAgent(obs_dim, act_dim, rew_dim,safety_layer=safety_layer, control_type=control_type,seed=seed,mlp_arch=mlp_arch,agent_config=agent_config, const_std=args.const_std, std_lb=args.std_lb)\n",
    "\n",
    "else:\n",
    "    agent = PPOAgent(obs_dim, act_dim, rew_dim,safety_layer=safety_layer, control_type=control_type,seed=seed,mlp_arch=mlp_arch,agent_config=agent_config)\n",
    "\n",
    "# build models\n",
    "agent.policy_model.build(input_shape=[None, trace_len, obs_dim])\n",
    "agent.value_model.build(input_shape=[None, trace_len, obs_dim])\n",
    "\n",
    "\n",
    "# # load optimizer states. This step must be prior to loading the model weights. See more https://stackoverflow.com/questions/49503748/save-and-load-model-optimizer-state\n",
    "# SAVED_OPT_W = None\n",
    "# with open(MODEL_PATH + SAVED_OPT_W, 'rb') as file:\n",
    "#     opt_states = pickle.load(file)\n",
    "# policy_opt_states = opt_states['policy']\n",
    "# value_opt_states = opt_states['value']\n",
    "# lag_opt_states = opt_states['lagrange']\n",
    "\n",
    "\n",
    "policy_weights = agent.policy_model.trainable_weights\n",
    "value_weights = agent.value_model.trainable_weights\n",
    "lagrange_multipliers = tf.Variable(initial_value=np.asarray([[args.lam_init,args.lam_init]]), trainable=True, dtype=tf.float32,constraint=lambda x: tf.clip_by_value(x, 0.0, np.inf))\n",
    "\n",
    "zero_grads_policy = [tf.zeros_like(w) for w in policy_weights]\n",
    "zero_grads_value = [tf.zeros_like(w) for w in value_weights]\n",
    "zero_grads_lag = [tf.zeros_like(w) for w in lagrange_multipliers]\n",
    "\n",
    "\n",
    "agent.optimizer_pi.apply_gradients(zip(zero_grads_policy, policy_weights))\n",
    "agent.optimizer_v.apply_gradients(zip(zero_grads_value, value_weights))\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=args.lam_lr,\n",
    "                                                            decay_steps=10000,decay_rate=0.9)\n",
    "optimizer_lagrange = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "optimizer_lagrange.apply_gradients(zip([zero_grads_lag], [lagrange_multipliers]))\n",
    "\n",
    "\n",
    "# load model weights and learned Lagrangians\n",
    "SAVED_W = 'model_weights_epoch_400'\n",
    "with open(MODEL_PATH + SAVED_W, 'rb') as file:\n",
    "    weights = pickle.load(file)\n",
    "\n",
    "\n",
    "# load optimizer states. This step must be prior to loading the model weights. See more https://stackoverflow.com/questions/49503748/save-and-load-model-optimizer-state\n",
    "\n",
    "agent.optimizer_pi.set_weights(weights['pi_opt_state'])\n",
    "agent.optimizer_v.set_weights(weights['v_opt_state'])\n",
    "optimizer_lagrange.set_weights(weights['lag_opt_state'])\n",
    "\n",
    "agent.policy_model.set_weights(weights['policy'])\n",
    "agent.value_model.set_weights(weights['value'])\n",
    "\n",
    "lags = weights['lags'].ravel()\n",
    "lagrange_multipliers.assign(np.asarray([lags[0], lags[1]]).reshape(1, -1)) # Replace None with the saved Lagrangians.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continued training begins...\n",
    "epochs = agent.agent_config['epochs']\n",
    "steps_per_epoch= agent.agent_config['steps_per_epoch']\n",
    "max_ep_len = agent.agent_config['max_ep_len']\n",
    "\n",
    "save_freq = args.savefreq\n",
    "\n",
    "if proc_id()==0:\n",
    "    config = dict()\n",
    "    config['arch'] = mlp_arch\n",
    "    config['agent'] = agent_config\n",
    "    config['args'] = args\n",
    "    with open(path+'/config','wb') as file:\n",
    "        pickle.dump(config,file)\n",
    "\n",
    "o, d, ep_ret, ep_len = env.reset(seed=proc_id()), False, np.asarray([0.0,0.0,0.0]), 0\n",
    "print('init state shape', o.shape)\n",
    "\n",
    "\n",
    "# sample another state for validation\n",
    "# o_val = val_env.reset()\n",
    "# val_ret = np.array([0., 0., 0.])\n",
    "\n",
    "if args.lstm_actor:\n",
    "    #Reset the recurrent layer's hidden state, 64 is number of output nodes in lstm\n",
    "    piLSTM_state = (tf.zeros((1, h_size)), tf.zeros((1, h_size)))\n",
    "    vLSTM_state =  (tf.zeros((1, h_size)), tf.zeros((1, h_size)))\n",
    "    # val_piLSTM_state =  (tf.zeros((1, h_size)), tf.zeros((1, h_size)))\n",
    "\n",
    "logger = dict()\n",
    "logger['pi_loss'] = []\n",
    "logger['kl_div'] =[]\n",
    "logger['mse_loss'] = []\n",
    "logger['ep_ret'] = []\n",
    "logger['ep_len'] = []\n",
    "logger['lagrange'] = []\n",
    "logger['entropy'] = []\n",
    "\n",
    "\n",
    "#syncing initial policy weights across actors. note that while env is deterministic, policy is stochastic\n",
    "o = np.asarray(o, dtype=np.float32).reshape(1,-1)   # coming from the env reset()\n",
    "cons_bounds=np.asarray(env.cons_bounds()).reshape(1,-1) \n",
    "if args.lstm_actor:\n",
    "    agent.output_actions(state=o.reshape(1,1,-1),cons_bounds=cons_bounds,lstm_state=piLSTM_state)\n",
    "else:\n",
    "    agent.output_actions(state=o,cons_bounds=cons_bounds) # return the action and lop-probability of the action.\n",
    "\n",
    "if proc_id()==0:\n",
    "        weights_pi = agent.policy_model.get_weights()\n",
    "        weights_v = agent.value_model.get_weights()\n",
    "else:\n",
    "    weights_pi = None \n",
    "    weights_v = None\n",
    "weights_pi = MPI.COMM_WORLD.bcast(weights_pi, root=0)\n",
    "weights_v = MPI.COMM_WORLD.bcast(weights_v, root=0)\n",
    "agent.policy_model.set_weights(weights_pi)\n",
    "agent.value_model.set_weights(weights_v)\n",
    "\n",
    "EXPID = args.expid\n",
    "logger['lagrange'].append(lagrange_multipliers.numpy().ravel())\n",
    "###############################\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "stime = time.time()\n",
    "# best_reward = -np.inf\n",
    "for epoch in (range(epochs)):\n",
    "    #print(\"Epoch {}\".format(epoch))\n",
    "    epoch_ep_ret = []\n",
    "    epoch_ep_len = []\n",
    "    epoch_entropy = []\n",
    "    # epoch_val_ret = []\n",
    "    if not args.learn_cost_critic:\n",
    "        state_costs = []\n",
    "        epoch_costs = []\n",
    "    if safety_layer['safety_level']>0:\n",
    "        cons_bounds_arr = []\n",
    "    pi_lstm_states = []\n",
    "    v_lstm_states= []\n",
    "    for t in tqdm(range(steps_per_epoch)):\n",
    "        if args.lstm_actor:\n",
    "            if t%trace_len ==0:\n",
    "                pi_lstm_states.append(piLSTM_state)\n",
    "                v_lstm_states.append(vLSTM_state)\n",
    "        o = np.asarray(o, dtype=np.float32).reshape(1,-1)\n",
    "        # o_val = np.asarray(o_val, dtype=np.float32).reshape(1,-1)\n",
    "        if args.lstm_actor:\n",
    "            if safety_layer['safety_level']==0:\n",
    "                a, logp_t,piLSTM_state, entropy = agent.output_actions(state=o.reshape(1,1,-1),lstm_state=piLSTM_state)\n",
    "                # a_val, val_logp_t, val_piLSTM_state = agent.output_actions(state=o_val.reshape(1,1,-1),\n",
    "                                                    # lstm_state=val_piLSTM_state)\n",
    "\n",
    "            else:\n",
    "                cons_bounds = env.cons_bounds()\n",
    "                cons_bounds_arr.append(cons_bounds)\n",
    "                a, logp_t,piLSTM_state, entropy = agent.output_actions(state=o.reshape(1,1,-1),cons_bounds=cons_bounds.reshape(1,-1),\n",
    "                                                            lstm_state=piLSTM_state)\n",
    "                # a_val, val_logp_t, val_piLSTM_state = agent.output_actions(state=o_val.reshape(1,1,-1), cons_bounds=cons_bounds.reshape(1,-1),\n",
    "                #                                     lstm_state=val_piLSTM_state)\n",
    "\n",
    "\n",
    "            v_t,vLSTM_state = agent.value_model(o.reshape(1,1,-1),lstm_state=vLSTM_state)\n",
    "            v_t = v_t.numpy().ravel()\n",
    "        else:\n",
    "            # if MLP use the action limiter\n",
    "            a_hist = [tf.constant([[0.0]])]\n",
    "            if safety_layer['safety_level']==0:\n",
    "                a_limit = tf.constant([[1.5 * 0.0016]])# 1.5 times the initial change\n",
    "                a, logp_t, entropy = agent.output_actions(state=o.reshape(1,-1)) # here can implement the action limiter: bounded by 1.5 times the initial decrease/increase\n",
    "                if args.limiter == 'True':\n",
    "                    if a - a_hist[-1] > a_limit:\n",
    "                        a = a_hist[-1] + a_limit\n",
    "                    elif a - a_hist[-1] < -a_limit:\n",
    "                        a = a_hist[-1] - a_limit\n",
    "                a_hist.append(a)\n",
    "            else:\n",
    "                cons_bounds = env.cons_bounds()\n",
    "                cons_bounds_arr.append(cons_bounds)\n",
    "                a, logp_t, entropy = agent.output_actions(state=o.reshape(1,-1),cons_bounds=cons_bounds.reshape(1,-1))\n",
    "            v_t = agent.value_model(o.reshape(1,-1)).numpy().ravel()\n",
    "\n",
    "        o2, rew_vec, d = env.step(a[0].numpy()) # get reward and new state from the env based on the output action.\n",
    "        ep_ret += rew_vec\n",
    "        ep_len += 1\n",
    "\n",
    "        # validation rewards\n",
    "        # o2_val, val_rew_vec, val_d = val_env.step(a_val[0].numpy())\n",
    "        # val_ret += val_rew_vec\n",
    "        #primal dual approach on penalized reward function\n",
    "        if safety_layer['safety_level']==0:\n",
    "            if args.learn_cost_critic:\n",
    "                rew_vec[0] = rew_vec[0] - lagrange_multipliers.numpy()[0][0]*rew_vec[1] - lagrange_multipliers.numpy()[0][1]*rew_vec[2]\n",
    "                penalized_rew = rew_vec\n",
    "            else:\n",
    "                state_costs.append(rew_vec[1:])\n",
    "                penalized_rew = np.asarray([rew_vec[0] - lagrange_multipliers.numpy()[0][0]*rew_vec[1] - lagrange_multipliers.numpy()[0][1]*rew_vec[2]])\n",
    "        else:\n",
    "            penalized_rew = rew_vec[0]\n",
    "\n",
    "        # save and log\n",
    "        agent.buf.store(o, a, penalized_rew, v_t, logp_t.numpy())\n",
    "\n",
    "        # Update obs (critical!)\n",
    "        o = o2\n",
    "        # o_val = o2_val\n",
    "\n",
    "        terminal = d or (ep_len == max_ep_len)\n",
    "        if terminal or (t==steps_per_epoch-1):\n",
    "            if not(terminal):\n",
    "                print('Warning: trajectory cut off by epoch at %d steps.'%ep_len)\n",
    "            # if trajectory didn't reach terminal state, bootstrap value target\n",
    "\n",
    "            if args.learn_cost_critic:\n",
    "                if args.lstm_actor:\n",
    "                    last_val = rew_vec.reshape(1,-1) if d else tf.squeeze(agent.value_model(o.reshape(1,1,-1),vLSTM_state)[0],axis=-1).numpy()\n",
    "                else:\n",
    "                    last_val = rew_vec.reshape(1,-1) if d else tf.squeeze(agent.value_model(o.reshape(1,-1)),axis=-1).numpy()\n",
    "            else:\n",
    "                if args.lstm_actor:\n",
    "                    last_val = rew_vec if d else tf.squeeze(agent.value_model(o.reshape(1,1,-1),vLSTM_state)[0],axis=-1).numpy()\n",
    "                else:\n",
    "                    last_val = rew_vec if d else tf.squeeze(agent.value_model(o.reshape(1,-1)),axis=-1).numpy()\n",
    "\n",
    "                if d:\n",
    "                    if safety_layer['safety_level'] == 0:\n",
    "                        last_val = np.asarray([last_val[0] - lagrange_multipliers.numpy()[0][0]*last_val[1] - lagrange_multipliers.numpy()[0][1]*last_val[2]]).reshape(1,-1)\n",
    "                    else:\n",
    "                        last_val = rew_vec[0]\n",
    "                        last_val = np.asarray([[last_val]])\n",
    "            agent.buf.finish_path(last_val)\n",
    "            if args.lstm_actor:\n",
    "                #Reset the recurrent layer's hidden state\n",
    "                piLSTM_state = (tf.zeros((1, h_size)), tf.zeros((1, h_size)))\n",
    "                vLSTM_state =  (tf.zeros((1, h_size)), tf.zeros((1, h_size)))\n",
    "                # val_piLSTM_state =  (tf.zeros((1, h_size)), tf.zeros((1, h_size)))\n",
    "            if terminal:\n",
    "                # only save EpRet / EpLen if trajectory finished\n",
    "                epoch_ep_ret.append(ep_ret)\n",
    "                epoch_ep_len.append(ep_len)\n",
    "                epoch_entropy.append(entropy)\n",
    "                # epoch_val_ret.append(val_ret)\n",
    "                if not args.learn_cost_critic and safety_layer['safety_level']==0:\n",
    "                    state_costs = np.asarray(state_costs)\n",
    "                    cost1_grad = np.mean(agent.buf.discount_cumsum(state_costs[:,0],discount=args.gamma))\n",
    "                    cost2_grad = np.mean(agent.buf.discount_cumsum(state_costs[:,1],discount=args.gamma))\n",
    "                    epoch_costs.append(np.asarray([cost1_grad,cost2_grad]))\n",
    "\n",
    "            if not args.learn_cost_critic and safety_layer['safety_level']==0:\n",
    "                state_costs = []\n",
    "            ep_ret, ep_len = np.asarray([0.0,0.0,0.0]), 0\n",
    "            o,  d = env.reset(seed=epoch+proc_id()),  False\n",
    "            # o_val = val_env.reset()\n",
    "\n",
    "\n",
    "    # Perform PPO update!\n",
    "    data = agent.buf.get()\n",
    "    inputs = dict()\n",
    "    if args.lstm_actor:\n",
    "        inputs['state'] = data[0].reshape(batch_size,trace_len,obs_dim)\n",
    "    else:\n",
    "        inputs['state'] = data[0]\n",
    "    inputs['action'] = data[1]\n",
    "    inputs['advs'] = data[2]\n",
    "    inputs['logp'] = data[-1]\n",
    "    inputs['ret'] = data[3]\n",
    "    if safety_layer['safety_level'] >0:\n",
    "        inputs['cons_bounds'] = np.asarray(cons_bounds_arr).reshape(1,-1)\n",
    "\n",
    "    if args.lstm_actor:\n",
    "        pi_hidden_states = tf.concat([pi_lstm_state[0] for pi_lstm_state in pi_lstm_states],axis=0)\n",
    "        pi_cell_states = tf.concat([pi_lstm_state[1] for pi_lstm_state in pi_lstm_states],axis=0)\n",
    "        v_hidden_states = tf.concat([v_lstm_state[0] for v_lstm_state in v_lstm_states],axis=0)\n",
    "        v_cell_states = tf.concat([v_lstm_state[1] for v_lstm_state in v_lstm_states],axis=0)\n",
    "        pi_lstm_states = (pi_hidden_states,pi_cell_states)\n",
    "        v_lstm_states = (v_hidden_states,v_cell_states)\n",
    "\n",
    "    mean_return = np.mean(epoch_ep_ret,axis=0)\n",
    "    mean_return = mpi_avg(mean_return)\n",
    "    mean_return = np.asarray(mean_return)\n",
    "\n",
    "    mean_entropy = np.mean(epoch_entropy, axis=0)\n",
    "    mean_entropy = mpi_avg(mean_entropy)\n",
    "    mean_entropy = np.asarray(mean_entropy)\n",
    "\n",
    "    # mean_val_return = np.mean(epoch_val_ret, axis=0)\n",
    "    # mean_val_return = mpi_avg(mean_val_return)\n",
    "    # mean_val_return = np.asarray(mean_val_return)\n",
    "\n",
    "    if safety_layer['safety_level']==0:\n",
    "        if args.learn_cost_critic:\n",
    "            #implementing primal dual variable approach\n",
    "            #notice how we sliced dim 0 to be 0:1, because we want the initial state distribution, not the steady-state distribution\n",
    "            #slicing to get initial distribution yields worse policy. use steady state distr instead\n",
    "            if args.lstm_actor:\n",
    "                critic_cost = tf.squeeze(agent.value_model(inputs['state'],lstm_state=v_lstm_states)[0])[:,1:]\n",
    "            else:\n",
    "                critic_cost = tf.squeeze(agent.value_model(inputs['state']))[:,1:] # [:,1:]\n",
    "            #clipping analyzed in https://arxiv.org/pdf/2205.11814.pdf\n",
    "            cost_function =tf.clip_by_value( tf.reshape(tf.reduce_mean(delta-critic_cost,axis=0),shape=(-1,1)), -np.inf, 0.0)\n",
    "            cost_function = mpi_avg(cost_function)\n",
    "        else:\n",
    "            epoch_costs = np.mean(epoch_costs,axis=0)\n",
    "            #implementing primal dual variable approach\n",
    "            #clip by value is from your JSAC paper\n",
    "            cost_function = tf.clip_by_value(tf.reshape(tf.convert_to_tensor(delta-epoch_costs),shape=(-1,1)), -np.inf, 0.0)\n",
    "            cost_function = mpi_avg(cost_function)\n",
    "        with tf.GradientTape() as tape:\n",
    "            lag_loss = tf.matmul(lagrange_multipliers,cost_function)\n",
    "        #this gradient is actually your dO(u)/du gradient in your TNNLS paper.\n",
    "        lag_gradients = tape.gradient(lag_loss, lagrange_multipliers)\n",
    "        optimizer_lagrange.apply_gradients(zip([lag_gradients], [lagrange_multipliers]))\n",
    "        ##############################################################\n",
    "\n",
    "\n",
    "\n",
    "    mean_length = np.mean(epoch_ep_len)\n",
    "    mean_length= mpi_avg(mean_length)\n",
    "    if args.lstm_actor:\n",
    "        loss_pi,approx_kl,mse = agent.update(inputs,pi_lstm_states,v_lstm_states)\n",
    "    else:\n",
    "        loss_pi,approx_kl,mse = agent.update(inputs)\n",
    "    logger['pi_loss'].append(loss_pi)\n",
    "    logger['kl_div'].append(approx_kl)\n",
    "    logger['mse_loss'].append(mse)\n",
    "    logger['ep_ret'].append(mean_return)\n",
    "    logger['ep_len'].append(mean_length)\n",
    "    logger['entropy'].append(mean_entropy)\n",
    "    logger['lagrange'].append(lagrange_multipliers.numpy().ravel())\n",
    "    # logger['val_ret'].append(mean_val_return)\n",
    "\n",
    "    if proc_id()==0:\n",
    "        if safety_layer['safety_level']==0:\n",
    "            print(\"Time {:10.2f}, Epoch {}, Ep Return {}, Lagrange {}, Grad {}\".format(time.time()-stime,epoch,mean_return,lagrange_multipliers.numpy(), cost_function.ravel()), flush=True)\n",
    "        else:\n",
    "            print(\"Time {:10.2f}, Epoch {}, Ep Return {}\".format(time.time()-stime,epoch,mean_return), flush=True)\n",
    "\n",
    "    if (epoch+1)%save_freq==0 and proc_id()==0:\n",
    "        weights_policy = agent.policy_model.get_weights()\n",
    "        weigths_val = agent.value_model.get_weights() # save the value network\n",
    "        weights = {'policy': weights_policy, 'value': weigths_val}\n",
    "        with open(path+'/model_weights_epoch_'+str(epoch+1), 'wb') as file:\n",
    "            pickle.dump(weights,file)\n",
    "        with open(path+'/logger', 'wb') as file:\n",
    "            pickle.dump(logger,file)\n",
    "\n",
    "\n",
    "\n",
    "if proc_id()==0:\n",
    "    log_data = np.asarray(logger['ep_ret'])\n",
    "    fig,axis =  plt.subplots(2,2,figsize=(10,10))\n",
    "    axis[0,0].plot(log_data[:,0])\n",
    "    axis[0,0].set_title('Rewards')\n",
    "    axis[0,1].plot(log_data[:,1])\n",
    "    axis[0,1].set_title('Cost 1')\n",
    "    axis[1,0].plot(log_data[:,2])\n",
    "    axis[1,0].set_title('Cost 2')\n",
    "    lagrange_data = np.asarray(logger['lagrange'])\n",
    "    axis[1,1].plot(lagrange_data[:,0],label='Lagrange mul 1')\n",
    "    axis[1,1].plot(lagrange_data[:,1],label='Lagrange mul 2')\n",
    "    axis[1,1].legend()\n",
    "    plt.savefig(path+'/learning_curves.pdf')\n",
    "#save final neural network weights\n",
    "if proc_id()==0:\n",
    "    weights_policy = agent.policy_model.get_weights()\n",
    "    weigths_val = agent.value_model.get_weights() # save the value network\n",
    "    weights = {'policy': weights_policy, 'value': weigths_val}\n",
    "    with open(path+'/model_weights_final', 'wb') as file:\n",
    "        pickle.dump(weights,file)\n",
    "    with open(path+'/logger', 'wb') as file:\n",
    "        pickle.dump(logger,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
